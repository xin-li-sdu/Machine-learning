## 90题细品吴恩达《机器学习》

### 第 1 题

一个计算机程序从经验E中学习任务T，并用P来衡量表现。并且，T的表现P随着经验E的增加而提高。
假设我们给一个学习算法输入了很多历史天气的数据，让它学会预测天气。什么是P的合理选择？

A. 计算大量历史气象数据的过程
B. 以上都不
**C. 正确预测未来日期天气的概率**
D. 天气预报任务

### 第 2 题

假设你正在做天气预报，并使用算法预测明天气温（摄氏度/华氏度），你会把这当作一个分类问题还是一个回归问题？

A. 分类
**B. 回归**

### 第 3 题

假设你在做股市预测。你想预测某家公司是否会在未来7天内宣布破产（通过对之前面临破产风险的类似公司的数据进行训练）。你会把这当作一个分类问题还是一个回归问题？

**A. 分类**
B. 回归

### 第 4 题

下面的一些问题最好使用有监督的学习算法来解决，而其他问题则应该使用无监督的学习算法来解决。以下哪一项你会使用监督学习？（选择所有适用的选项）在每种情况下，假设有适当的数据集可供算法学习。

**A. 根据一个人的基因（DNA）数据，预测他/她的未来10年患糖尿病的几率**

B. 根据心脏病患者的大量医疗记录数据集，尝试了解是否有不同类患者群，我们可以为其量身定制不同的治疗方案

**C. 让计算机检查一段音频，并对该音频中是否有人声（即人声歌唱）或是否只有乐器（而没有人声）进行分类**

D. 给出1000名医疗患者对实验药物的反应（如治疗效果、副作用等）的数据，发现患者对药物的反应是否有不同的类别或“类型”，如果有，这些类别是什么

**解释：假设了每种情况都有训练集，则A为训练其他人基因-有无糖尿病的示例来对一个人进行预测。B，D显然没有明确有哪些类别，需要用到聚类或者其他的方法进行分类**

### 第 5 题

哪一个是机器学习的合理定义？

A. 机器学习从标记的数据中学习

**B. 机器学习能使计算机能够在没有明确编程的情况下学习**

C. 机器学习是计算机编程的科学

D. 机器学习是允许机器人智能行动的领域

### 第 6 题

基于一个学生在大学一年级的表现，预测他在大学二年级表现。
令x等于学生在大学第一年得到的“A”的个数（包括A-，A和A+成绩）学生在大学第一年得到的成绩。预测y的值：第二年获得的“A”级的数量
这里每一行是一个训练数据。在线性回归中，我们的假设，并且我们使用m来表示训练示例的数量。

```
| x    | y    |  
| 3    | 2    |  
| 1    | 2    |  
| 0    | 1    |  
| 4    | 3    |  
```

对于上面给出的训练集，m的值是多少？

**答：m表示训练集数量，4行**

### 第 7 题

对于这个问题，假设我们使用第一题中的训练集。并且，我们对代价函数的定义是
$$
J(\theta_0,\theta_1) =\frac{1}{2m}\sum_{i=1}^m{(h_\theta(x^{(i)})-y^{(i)})^2}
$$
求J(0,1)

**答：0.5**

### 第 8 题

问题1中，线性回归假设的两个参数为-1， 2，求预测值h(6)？

**答：-1+2*6  = 11**

### 第 9 题

代价函数与参数的关系如图2所示。“图1”中给出了相同代价函数的等高线图。根据图示，选择正确的选项（选出所有正确项）

![Image Name](https://cdn.kesci.com/upload/image/q3o1ai1t0.png?imageView2/0/w/960/h/960)

**A. 从B点开始，学习率合适的梯度下降算法会最终帮助我们到达或者接近A点，即代价函数在A点有最小值**

B. 点P（图2的全局最小值）对应于图1的点C

C. 从B点开始，学习率合适的梯度下降算法会最终帮助我们到达或者接近C点，即代价函数在C点有最小值

D. 从B点开始，学习率合适的梯度下降算法会最终帮助我们到达或者接近A点，即代价函数在A点有最大值

**E. 点P（图2的全局最小值）对应于图1的点A**

### 第 10 题

假设对于某个线性回归问题（比如预测房价），我们有一些训练集，对于我们的训练集，我们能够找到一些，使得
$$
J(\theta_0.\theta_1)=0
$$

以下哪项陈述是正确的？（选出所有正确项）

A. 为了实现这一点，我们必须有两个参数都为0，这样才能使J = 0

**B. 对于满足的J=0的两个参数的值，其对于每个训练例子，都有预测值等于实际值**

C. 这是不可能的：通过J的定义，不可能存在两个参数使得J=0

D. 即使对于我们还没有看到的新例子，我们也可以完美地预测y的值（例如，我们可以完美地预测我们尚未见过的新房的价格）

### 第 11 题

定义2个矩阵
$$
A = \left[   
\matrix{
  4 & 3\\
  6 & 9\\
}
\right]
B=\left[
\matrix{
-2&9\\
-5&2\\
}
\right]
$$
那么A-B是多少？

A.
$$
\left[   
\matrix{
  4 & 12\\
  1 & 1\\
}
\right]
$$

B.
$$
\left[   
\matrix{
  6 & -12\\
  11 & 11\\
}
\right]
$$
C.
$$
\left[   
\matrix{
  2 & -6\\
  1 & 7\\
}
\right]
$$
**D.**
$$
\left[   
\matrix{
  6 & -6\\
  11 & 7\\
}
\right]
$$

### 第 12 题

令
$$
x=\left[
\matrix{
2\\7\\4\\1
}
\right]
$$
那么1/2 * x是多少

答：
$$
x=\left[
\matrix{
1\\3.5\\2\\0.5
}
\right]
$$

### 第 13 题

令u是一个3维向量，并且
$$
u=\left[
\matrix{
5\\1\\9
}
\right]
$$
那么u的转置是多少
$$
x=\left[
\matrix{
5&1&9
}
\right]
$$

### 第 14 题

令u，v为3维向量，并且
$$
u=\left[
\matrix{
1\\2\\-1
}
\right]
v=\left[
\matrix{
2\\2\\4
}
\right]
$$
那么u的转置乘v是多少？

**答：2**

### 第 15 题

令A和B是3x3矩阵，以下哪一项一定是正确的（选出所有正确项）

**A. A+B = B+A**
**B. 如果v是一个3维向量，那么A* B * v是三维向量**
C. A * B  * A=B * A * B
D. 如果C=A*B，那么C是个6x6矩阵

### 第 16 题

假设m=4个学生上了一节课，有期中考试和期末考试。你已经收集了他们在两次考试中的分数数据集，如下所示：

| 期中得分 | (期中得分)^2 | 期末得分 |
| :------- | :----------- | :------- |
| 89       | 7921         | 96       |
| 72       | 5184         | 74       |
| 94       | 8836         | 87       |
| 69       | 4761         | 78       |

你想用多项式回归来预测一个学生的期中考试成绩。具体地说，假设你想拟合一个的模型，其中x_1是期中得分，x_2是（期中得分）^2。此外，你计划同时使用特征缩放（除以特征的“最大值-最小值”或范围）和均值归一化。

标准化后x_2(4)的特征值是多少？（提示：期中=89，期末=96是训练示例1）

**答：-0.47**

### 第 17 题

用学习率=0.3进行15次梯度下降迭代，每次迭代后计算J。你会发现J的值下降缓慢，并且在15次迭代后仍在下降。基于此，以下哪个结论似乎最可信？

A. 0.3是学习率的有效选择。

B. 与其使用当前值，不如尝试更小的值（比如0.1）

**C. 与其使用当前值，不如尝试更大的值（比如1.0）**

### 第 18 题

假设您有m=14个训练示例，有n=3个特性（不包括需要另外添加的恒为1的截距项），正规方程是
$$
\theta=(X^TX)^{-1}X^Ty
$$
。对于给定m和n的值，这个方程中三个变量的维数分别是多少？

A. 14×3, 14×1, 3×3
**B. 14×4, 14×1, 4×1**(X和最终结果都有那个常数项)
C. 14×3, 14×1, 3×1
D. 14×4, 14×4, 4×4

### 第 19 题

假设您有一个数据集，每个示例有m=1000000个示例和n=200000个特性。你想用多元线性回归来拟合参数到我们的数据。你更应该用梯度下降还是正规方程？

**A. 梯度下降，因为正规方程中中计算非常慢**

B. 正规方程，因为它提供了一种直接求解的有效方法

C. 梯度下降，因为它总是收敛到最优

D. 正规方程，因为梯度下降可能无法找到最优

### 第 20 题

以下哪些是使用特征缩放的原因？

A. 它可以防止梯度下降陷入局部最优

B. 它通过降低梯度下降的每次迭代的计算成本来加速梯度下降

**C. 它通过减少迭代次数来获得一个好的解，从而加快了梯度下降的速度**

D. 它防止矩阵（用于正规方程）不可逆（奇异/退化）

### 第 21~25 题：Octave操作跳过

### 第 26题

假设您已经训练了一个逻辑分类器，它在一个新示例x上输出一个预测$h(\theta)=0.4$。这意味着（选出所有正确项）：

A. 我们对$P(y=0|x;\theta)$的估计是0.4

B. 我们对$P(y=1|x;\theta)$的估计是0.6

**C. 我们对$P(y=0|x;\theta)$的估计是0.6**

**D. 我们对$P(y=1|x;\theta)$的估计是0.4**

### 第 27 题

假设您有以下训练集，并拟合logistic回归分类器$h_θ(x)=g(θ_0+θ_1x_1+θ_2x_2)$

| $x_1$ | $x_2$ | y    |
| ----- | ----- | ---- |
| 1     | 0.5   | 0    |
| 1     | 1.5   | 0    |
| 2     | 1     | 1    |
| 3     | 1     | 0    |

![](https://tvax1.sinaimg.cn/large/005IQUPRly1glf4g46549j30l30d8glm.jpg)

以下哪项是正确的？选出所有正确项

**A. 添加多项式特征（例如，使用$h_θ(x)=g(θ_0+θ_1x_1+θ_2x_2+θ_3x_1^2+θ_4x_1x_2+θ_5x_2^2)）$可以增加我们拟合训练数据的程度**

**B. 在θ的最佳值（例如，由fminunc找到）处，$J(θ)≥0$**

C.添加多项式特征（例如，使用$h_θ(x)=g(θ_0+θ_1x_1+θ_2x_2+θ_3x_1^2+θ_4x_1x_2+θ_5x_2^2)$将增加J(θ)，因为我们现在正在对更多项进行求和

D.如果我们训练梯度下降迭代足够多次，对于训练集中的一些例子$x^{(i)}$，可能得到$h_θ(x^{(i)})>1$

### 第 28 题

对于逻辑回归，梯度由$\frac∂{∂θ_j}J(θ)=\frac1m\sum_{i=1}^m(h_θ(x^{(i)})−y^{(i)})x_j^{(i)}$给出。以下哪项是学习率为α的逻辑回归的正确梯度下降更新？选出所有正确项

A. $θ:=θ−α\frac1m∑_{i=1}^m(θ^Tx−y^{(i)})x^{(i)}$

**B. $θ_j:=θ_j−α\frac1m∑_{i=1}^m(\frac1{1+e^{-\theta^Tx^{(i)}}}−y^{(i)})x^{(i)}$（同时更新所有j）**

C. $θ:=θ−α\frac1m∑_{i=1}^m(h_\theta(x^{(i)})−y^{(i)})x^{(i)}$（同时更新所有j）

**D. $θ:=θ−α\frac1m∑_{i=1}^m(h_\theta(x^{(i)})−y^{(i)})x_j^{(i)}$（同时更新所有j）**

### 第 29 题

以下哪项陈述是正确的？选出所有正确项

A. 对于逻辑回归，梯度下降有时会收敛到一个局部最小值（并且无法找到全局最小值）。这就是为什么我们更喜欢更先进的优化算法，如fminunc（共轭梯度/BFGS/L-BFGS/等等）

**B. sigmoid函数$g(z)=\frac1{1+e^{(-z)}}$数值永远不会大于1**

**C.用m≥1个例子训练的逻辑回归的代价函数J(θ)总是大于或等于零**

D. 使用线性回归+阈值的方法做分类预测，总是很有效的

### 第 30 题

假设训练一个逻辑回归分类器$h_θ(x)=g(θ_0+θ_1x_1+θ_2x_2)$。假设$θ_0=6,θ_1=−1,θ_2=0$，下列哪个图表示分类器找到的决策边界？

**A.**

![Image Name](https://cdn.kesci.com/upload/image/q3oeqa1yek.jpg?imageView2/0/w/960/h/960)

B.

![Image Name](https://cdn.kesci.com/upload/image/q3oeqo1xpd.jpg?imageView2/0/w/960/h/960)

C.

![Image Name](https://cdn.kesci.com/upload/image/q3oeqw5knk.jpg?imageView2/0/w/960/h/960)

D.

![Image Name](https://cdn.kesci.com/upload/image/q3oesexzdg.jpg?imageView2/0/w/960/h/960)

### 第 31 题

你正在训练一个分类逻辑回归模型。以下哪项陈述是正确的？选出所有正确项

A. 将正则化引入到模型中，总是能在训练集上获得相同或更好的性能

B. 在模型中添加许多新特性有助于防止训练集过度拟合

C. 将正则化引入到模型中，对于训练集中没有的例子，总是可以获得相同或更好的性能

**D. 向模型中添加新特征总是会在训练集上获得相同或更好的性能**

### 第 32 题

假设您进行了两次逻辑回归，一次是$λ=0$，一次是$λ=1$。其中一次，得到参数$θ=\left[\matrix{81.47\\12.69}\right]$，另一次，得到$θ=\left[\matrix{13.01\\0.91}\right]$。
但是，您忘记了哪个λ值对应于哪个θ值。你认为哪个对应于λ=1？

**A.**$θ=\left[\matrix{13.01\\0.91}\right]$

B.$θ=\left[\matrix{81.47\\12.69}\right]$

### 第 33 题(⭐)

以下关于正则化的陈述哪一个是正确的？选出所有正确项

A. 使用太大的λ值可能会导致您的假设与数据过拟合；这可以通过减小λ来避免

B. 使用非常大的值λ不会影响假设的性能；我们不将λ设置为太大的唯一原因是避免数值问题

C. 考虑一个分类问题。添加正则化可能会导致分类器错误地分类某些训练示例（当不使用正则化时，即当λ=0时，它正确地分类了这些示例）

D. 由于逻辑回归的输出值$0≤h_θ(x)≤1$，其输出值的范围无论如何只能通过正则化来“缩小”一点，因此正则化通常对其没有帮助

### 第 34 题

下列哪一个图片的假设与训练集过拟合？

**A.**

![Image Name](https://cdn.kesci.com/upload/image/q3oh2j8ms5.jpg?imageView2/0/w/960/h/960)

B.

![Image Name](https://cdn.kesci.com/upload/image/q3oh2pcz0h.jpg?imageView2/0/w/960/h/960)

C.

![Image Name](https://cdn.kesci.com/upload/image/q3oh2w1vtg.jpg?imageView2/0/w/960/h/960)

D.

![Image Name](https://cdn.kesci.com/upload/image/q3oh34y2z3.jpg?imageView2/0/w/960/h/960)

### 第 35 题

下列哪一个图片的假设与训练集欠拟合？

**A.**

![Image Name](https://cdn.kesci.com/upload/image/q3oh7rde8b.jpg?imageView2/0/w/960/h/960)

B.

![Image Name](https://cdn.kesci.com/upload/image/q3oh7xnr0x.jpg?imageView2/0/w/960/h/960)

C.

![Image Name](https://cdn.kesci.com/upload/image/q3oh8393qg.jpg?imageView2/0/w/960/h/960)

D.

![Image Name](https://cdn.kesci.com/upload/image/q3oh8dlzjj.jpg?imageView2/0/w/960/h/960)

### 第 36 题(⭐D)

以下哪项陈述是正确的？选择所有正确项

**A. 神经网络中隐藏单元的激活值，在应用了sigmoid函数之后，总是在（0，1）范围内**

**B. 在二进制值（0或1）上的逻辑函数可以（近似）用一些神经网络来表示**

C. 两层（一个输入层，一个输出层，没有隐藏层）神经网络可以表示异或函数

D. 假设有一个三个类的多类分类问题，使用三层网络进行训练。设$a_1^{(3)}=(h_Θ^{(x)})_1$为第一输出单元的激活，并且类似地，有$a_2^{(3)}=(h_Θ(x))_2$和$a_3^{(3)}=(h_Θ(x))_3$。那么对于任何输入x，必须有$a_1^{(3)}+a_2^{(3)}+a_3^{(3)}=1$

### 第 37 题

考虑以下两个二值输入$x1,x2∈{\{0,1}\}$和输出$h_Θ(x)$的神经网络。它（近似）计算了下列哪一个逻辑函数？

![Image Name](https://cdn.kesci.com/upload/image/q3oj8j5iry.png?imageView2/0/w/960/h/960)

**A. OR**
B. AND
C. NAND (与非)
D. XOR (异或)

### 第 38 题

考虑下面给出的神经网络。下列哪个方程正确地计算了$a_1^{(3)}$的激活？注：g(z)是sigmoid激活函数

![Image Name](https://cdn.kesci.com/upload/image/q3ojbwnojc.jpg?imageView2/0/w/960/h/960)

**A.**$a_1^{(3)}=g(\Theta_{1,0}^{(2)}a_{0}^{(2)}+\Theta_{1,1}^{(2)}a_{1}^{(2)}+\Theta_{1,2}^{(2)}a_{2}^{(2)})$

A.$a_1^{(3)}=g(\Theta_{1,0}^{(1)}a_{0}^{(1)}+\Theta_{1,1}^{(1)}a_{1}^{(1)}+\Theta_{1,2}^{(1)}a_{2}^{(1)})$

A.$a_1^{(3)}=g(\Theta_{1,0}^{(1)}a_{0}^{(2)}+\Theta_{1,1}^{(1)}a_{1}^{(2)}+\Theta_{1,2}^{(1)}a_{2}^{(2)})$

D.此网络中不存在激活$a_1^{(3)}$

### 第 39 题 涉及octave跳过

### 第 40 题(⭐)

您正在使用下图所示的神经网络，并已学习参数$Θ^{(1)}=\left[\matrix{1&1&2.4\\1&1.7&3.2}\right]$（用于计算$a^{(2)}$）和$Θ(2)=\left[\matrix{1&0.3&-1.2}\right]$（用于作用在$a^{(2)}$的函数，计算$a_{(3)}$的值）。

假设您交换第一个隐藏层的2个单元的参数$Θ^{(1)}=\left[\matrix{1&1&3.2\\1&1.7&2.4}\right]$，并且还交换输出层$Θ(2)=\left[\matrix{1&-1.2&0.3}\right]$。这将如何改变输出hΘ(x)的值？

![Image Name](https://cdn.kesci.com/upload/image/q3ok7hvi53.jpg?imageView2/0/w/960/h/960)

**A. 不变**
B. 变大
C. 变小
D. 信息不全，可能变大也可能变小

### 第 41 题~45题（反向传播还没学会）

您正在训练一个三层神经网络，希望使用反向传播来计算代价函数的梯度。
在反向传播算法中，其中一个步骤是更新

$\Delta_{ij}^{(2)}:=\Delta_{ij}^{(2)}+\delta_{i}^{(3)}*{(a^{(2)})}_{j}$

对于每个i，j，下面哪一个是这个步骤的正确矢量化？

### 第 46 题

你训练一个学习算法，发现它在测试集上的误差很高。绘制学习曲线，并获得下图。算法是否存在高偏差、高方差或两者都不存在？

![Image Name](https://cdn.kesci.com/upload/image/q3pxb7r62x.jpg?imageView2/0/w/960/h/960)

**A. 高偏差**
B. 高方差
C. 两者都不

### 第 47 题

假设您已经实现了正则化逻辑回归来分类图像中的对象（即，还没有实现图像识别）。然而，当你在一组新的图像上检验你的模型时，你会发现它对新图像的预测有误差非常大。然而，你的假设在训练集上拟合的很好。以下哪个做法可以改善？选出所有正确项

A. 尝试添加多项式特征
**B. 获取更多训练示例**
**C. 尝试使用较少的特征**
D. 少用训练的例子

### 第 48 题

假设您已经实现了正则化的逻辑来预测客户将在购物网站上购买哪些商品。然而，当你在一组新的客户身上测试你的模型时，你发现它在预测中的误差很大。此外，该模型在训练集上表现不佳。以下哪个做法可以改善？选出所有正确项

**A. 尝试获取并使用其他特征**
**B. 尝试添加多项式特征**
C. 尝试使用较少的特征
D. 尝试增加正则化参数λ

### 第 49 题

以下哪项陈述是正确的？选出所有正确项

**A. 假设您正在训练一个正则化的线性回归模型。选择正则化参数λ值的推荐方法是选择交叉验证误差最小的λ值。**

B. 假设您正在训练一个正则化的线性回归模型。选择正则化参数λ值的推荐方法是选择给出最小测试集误差的λ值。

C. 假设你正在训练一个正则化线性回归模型，推荐的选择正则化参数λ值的方法是选择给出最小训练集误差的λ值。

**D. 学习算法在训练集上的性能通常比在测试集上的性能要好。**

### 第 50 题

以下哪项陈述是正确的？选出所有正确项

**A. 在调试学习算法时，绘制学习曲线有助于了解是否存在高偏差或高方差问题。**

**B. 如果一个学习算法受到高方差的影响，增加更多的训练实例可能会改善测试误差。**

C. 我们总是喜欢高方差的模型（而不是高偏差的模型），因为它们能够更好地适应训练集。

**D. 如果一个学习算法有很高的偏差，仅仅增加更多的训练实例可能不会显著改善测试误差**

### 第 51 题（太简单）

### 第 52 题（⭐）

假设一个庞大的数据集可以用来训练一个学习算法。当以下两个条件成立时，对大量数据进行训练可能会产生良好的性能。两个条件是哪两个？

**A. 特征x包含足够的信息来精确地预测y。（例如，一个验证这一点的方法是，当只给x时，人类专家是否能够自信地预测y）。**

B. 我们训练一个具有少量参数的学习算法（因此不太可能过拟合）。

**C. 我们训练具有大量参数的学习算法（能够学习/表示相当复杂的函数）。**

D. 我们训练一个不使用正则化的模型。

### 第 53 题

假设您已经训练了一个输出$h_θ(x)$的逻辑回归分类器。目前，如果$h_θ(x)≥threshold$，则预测1，如果$h_θ(x)≤threshold$，则预测0，当前阈值设置为0.5。

假设您将阈值增加到0.9。以下哪项是正确的？选出所有正确项

A. 现在分类器的精度可能更低。

B. 分类器的准确度和召回率可能不变，但准确度较低。

C. 分类器的准确度和召回率可能不变，但精度较高。

**D. 分类器现在可能具有较低的召回率。**

假设您将阈值降低到0.3。以下哪项是正确的？选出所有正确项

**A. 分类器现在可能具有更高的召回率。**

B. 分类器的准确度和召回率可能不变，但精度较高。

C. 分类器现在可能具有更高的精度。

D. 分类器的准确度和召回率可能不变，但准确度较低。

### 第 54 题（⭐⭐⭐）

假设您正在使用垃圾邮件分类器，其中垃圾邮件是正例（y=1），非垃圾邮件是反例（y=0）。您有一组电子邮件训练集，其中99%的电子邮件是非垃圾邮件，另1%是垃圾邮件。以下哪项陈述是正确的？选出所有正确项

A. 一个好的分类器应该在交叉验证集上同时具有高精度precision和高召回率recall。

B. 如果您总是预测非垃圾邮件（输出y=0），那么您的分类器在训练集上的准确度accuracy将达到99%，而且它在交叉验证集上的性能可能类似。

C. 如果您总是预测非垃圾邮件（输出y=0），那么您的分类器的准确度accuracy将达到99%。

D. 如果您总是预测非垃圾邮件（输出y=0），那么您的分类器在训练集上的准确度accuracy将达到99%，但在交叉验证集上的准确率会更差，因为它过拟合训练数据。

E. 如果总是预测垃圾邮件（输出y=1），则分类器的召回率recall为0%，精度precision为99%。

F. 如果总是预测非垃圾邮件（输出y=0），则分类器的召回率recall为0%。

G. 如果您总是预测垃圾邮件（输出y=1），那么您的分类器将具有召回率recall 100%和精度precision 1%。

H. 如果您总是预测非垃圾邮件（输出y=0），那么您的分类器的准确度accuracy将达到99%。

### 第 55 题

以下哪项陈述是正确的？选出所有正确项

A. 在构建学习算法的第一个版本之前，花大量时间收集大量数据是一个好主意。

**B. 在倾斜的数据集上（例如，当有更多的正面例子而不是负面例子时），准确度不是一个很好的性能度量，您应该根据准确度和召回率使用F1分数。**

C. 训练完逻辑回归分类器后，必须使用0.5作为预测示例是正是负的阈值。

**D. 使用一个非常大的训练集使得模型不太可能过度拟合训练数据。**

E. 如果您的模型不适合训练集，那么获取更多数据可能会有帮助。
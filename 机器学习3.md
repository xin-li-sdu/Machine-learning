# 机器学习-实验三

   

​                                                                                             李昕.202100460065

[TOC]



## 实验目的

在本作业使用 **linear regression** 预测出空气污染指数(即PM2.5) 的数值，通过实验理解线性回归模型，学会如何使用梯度下降法训练模型，了解不同学习率对模型训练的影响。

## 实验环境

| 操作系统   | 处理器      | 使用语言 | 编译器 |
| ---------- | ----------- | -------- | ------ |
| windows 11 | AMD Ryzen 7 | python3  | IDLE   |

## 实验方法

### 模型选择：线性回归模型

在本实验中，使用线性模型$y=wx+b$，其中x为本实验提供的18个特征的数据，通过下列公式
$$
\mathbf{Y} =\left[\begin{array}{c}   w_1 x_{1,1} ...+  w_n x_{1,n} +b\\  w_1 x_{2,1} ...+  w_n x_{2,n}  +b\\ \vdots \\w_1 x_{m,1} ...+  w_n x_{m,n} +b \end{array}\right]=\left[\begin{array}{c}  1 \quad x_{1,1} \quad ... \quad x_{1,n}\\ 1 \quad x_{2,1} \quad ... \quad x_{2,n}  \\ \vdots \\1 \quad x_{m,1} \quad ... \quad x_{m,n}  \end{array}\right]\left[\begin{array}{c} b \\ w_{1} \\ \vdots \\ w_{n} \end{array}\right]  =Xw
$$
将常数b作为全为 1 的一列添加到训练集前，权重向量 w 的第一个元素对应常数项 b，以此得到线性模型$Y=Xw$，代码如下：``x_train_set = np.concatenate((np.ones([len(x_train_set), 1]), x_train_set), axis = 1).astype(float)``

### $LOSS$函数选择:$RMSE$和$MSE$

$LOSS$函数采取了两种，第一种为采用均方根误差（$Root Mean Square Error$，$RMSE$）如下：
$$
L(\mathbf{w})=\sqrt{\frac{1}{m} \sum_{i=1}^{m}\left(\mathbf{x}_{i} \mathbf{w}-\hat{y}_{i}\right)^{2}}
$$
编写代码为：``loss = np.sqrt(np.sum(np.power(np.dot(x, w) - y, 2))/ len(x))``

第二种是采用平方根误差（$MSE$）如下：
$$
L(\mathbf{w})=\sum_{i=1}^{m}\left(\mathbf{x}_{i} \mathbf{w}-\hat{y}_{i}\right)^{2}
$$
编写代码为``loss = np.sum(np.power(np.dot(x_train_set, w) - y_train_set, 2)``

对应的下降梯度$g_t$,即$\frac{\partial L\left(\mathbf{w}^{t}\right)}{\partial b\left(\mathbf{w}^{t}\right)}$推导如下：
$$
 \frac{\partial L\left(\mathbf{w}^{t}\right)}{\partial b\left(\mathbf{w}^{t}\right)}= \left[\begin{array}{c} 2\left(\mathbf{x}_{1} \mathbf{w}-\hat{y}_{1}\right)+2\left(\mathbf{x}_{2} \mathbf{w}-\hat{y}_{2}\right)+\cdots+2\left(\mathbf{x}_{m} \mathbf{w}-\hat{y}_{m}\right) \\ 2x_{1,1}\left(\mathbf{x}_{1} \mathbf{w}-\hat{y}_{1}\right)+2x_{2,1}\left(\mathbf{x}_{2} \mathbf{w}-\hat{y}_{2}\right)+\cdots+2x_{m, 1}\left(\mathbf{x}_{m} \mathbf{w}-\hat{y}_{m}\right) \\ \vdots \\ 2x_{1, n}\left(\mathbf{x}_{1} \mathbf{w}-\hat{y}_{1}\right)+2x_{2, n}\left(\mathbf{x}_{2} \mathbf{w}-\hat{y}_{2}\right)+\cdots+2x_{m, n}\left(\mathbf{x}_{m} \mathbf{w}-\hat{y}_{m}\right) \end{array}\right]\\=2*\left[\begin{array}{cccc} 1 & 1 & \cdots & 1 \\ x_{1,1} & x_{2,1} & \cdots & x_{m, 1} \\ x_{1,2} & x_{2,2} & \cdots & x_{m, 2} \\ \vdots & \vdots & \ddots & \vdots \\ x_{1, n} & x_{2, n} & \cdots & x_{m, n} \end{array}\right]*\left(\left[\begin{array}{c} \mathbf{x}_{1} \mathbf{w} \\ \mathbf{x}_{2} \mathbf{w} \\ \vdots \\ \mathbf{x}_{m} \mathbf{w} \end{array}\right]-\left[\begin{array}{c} \hat{y}_{1} \\ \hat{y}_{2} \\ \vdots \\ \hat{y}_{m} \end{array}\right]\right)=2 \mathbf{X}^{T}(\mathbf{X} \mathbf{w}-\hat{\mathbf{y}})
			
					
$$
对应代码为（x_train_set为划分的训练集）：``gradient = 2 * np.dot(x_train_set.transpose(), np.dot(x_train_set, w) - y_train_set) ``

### 梯度下降算法选择：$Adagrad$

梯度下降算法采用$Adagrad$优化，公式如下：
$$
\mathbf{w}^{t+1}=\mathbf{w}^{t}-\frac{\eta}{\sqrt{\sum_{i=0}^{t}g_i^2+\epsilon}} *g_t
$$
其中$\eta$为设定的初始学习率，$\epsilon$是一个极小整数（避免$adagrad$的分母为0），该算法使得学习率可以动态调整，即如果目标函数中自变量偏导数较大，那么该元素的学习率将下降较快，反之，如果目标函数自变量偏导数较小，那么该元素的学习率将下降较慢，代码如下：

```python
adagrad += gradient ** 2       #adagrad 是历史梯度平方和的累加  
w = w - learning_rate[k] * gradient / np.sqrt(adagrad + eps) #w按照学习率和gadient梯度下降
```

### 交叉验证

在模型训练过程中，会出现在训练集上表现很好，但是测试集上表现很差的情况，同时不同的，模型和超参数设置也会影响最终预测PM2.5的效果，为了减少过拟合，提高模型的泛化能力，将已有数据集中保留一部分数据作为测试集，即将原有数据分为训练集train_set和验证集validation，train_set用来训练模型，validation用来验证调整模型。

```python
x_train_set = x[: math.floor(len(x) * 0.8), :]
y_train_set = y[: math.floor(len(y) * 0.8), :]
x_validation = x[math.floor(len(x) * 0.8): , :]
y_validation = y[math.floor(len(y) * 0.8): , :]
```

## 实验过程

### 特征提取&数据预处理

由于在**范例代码**中已经包含了此部分操作，故不再展示具体代码，下面对该步骤代码进行解析。

首先，数据载入和数据无效项预处理，取消原表格中的汉字和NR，将NR替换成0，然后设立sample（一个18*480的$NumPy$数组），用于存储一个月内20天（每天24小时）的数据。对于每个月，每10h分成一组，由前9h的数据来预测第10h的PM2.5，把前9h的18项指标数据放入x，把第10h的PM2.5数据放入y。设立滑动区间的大小为9，从第1个小时开始向右滑动。故每月都有480-9=471组这样的数据。

### 数据标准化

在训练前，将18个不同特征的数据缩放到相同的尺度，以便更好地进行训练，即使$x_{i,j}=\frac{x_{i,j}-\overline{x}}{σ}$,代码如下：

```python
mean_x = np.mean(x, axis = 0) #18 * 9 每一个特征的平均值
std_x = np.std(x, axis = 0) #18 * 9 每一个特征的标准差
for i in range(len(x)): 
#12 * 471  z-score标准化，通过（x-μ）/σ将两组或多组数据转化为无单位的Z-Score分值，使得数据标准统一化，提高了数据可比性，削弱了数据解释性
    for j in range(len(x[0])): #18 * 9 
        if std_x[j] != 0:
            x[i][j] = (x[i][j] - mean_x[j]) / std_x[j]
```



### 训练数据划分，便于交叉验证

在老师提供的范例代码中，训练数据按照 8:2 的比例被分成训练集train_set和验证集validation，其中train_set用于训练，而validation不会参与训练，用于验证。在本实验中，老师要求使用四种不同的学习率进行训练（其他参数需一致），我在后面的训练过程中，利用验证集在四种学习率中找到四种里最佳的学习率，并利用最好的学习率对整个训练数据进行重新训练，得到w。新增代码为：``pred_loss = np.sqrt(np.mean((np.dot(x_validation, w) - y_validation) ** 2))``

### 进行训练，并预测数据

训练部分需要补全的部分已在本报告**Part3 实验方法**中已经给出，即用$L=\sqrt{\frac{1}{m} \sum_{i=1}^{m}\left(\mathbf{x}_{i} \mathbf{w}-\hat{y}_{i}\right)^{2}}$作为$LOSS$函数，代码为``loss = np.sqrt(np.sum(np.power(np.dot(x, w) - y, 2))/ len(x))``，偏微分$g_t=2 \mathbf{X}^{T}(\mathbf{X} \mathbf{w}-\hat{\mathbf{y}})$为下降梯度，代码为``gradient = 2 * np.dot(x_train_set.transpose(), np.dot(x_train_set, w) - y_train_set) ``,梯度下降算法采用$Adagrad$优化，公式为$\mathbf{w}^{t+1}=\mathbf{w}^{t}-\frac{\eta}{\sqrt{\sum_{i=0}^{t}g_i^2+\epsilon}} *g_t$，代码为``w = w - learning_rate[k] * gradient / np.sqrt(adagrad + eps)``。设定迭代次数统一为4000次，四种学习率为0.01，0.1，1，10。

利用提前划分的训练集x_train_set来对四种学习率分别训练，并使用验证集x_validation进行交叉验证，得到最好（最终loss值最小）的一种学习率，再在完整的训练集上再此训练，得到w。

填充老师范例代码中的缺失部分，得到以下完整代码（本部分代码为利用训练集进行训练，交叉验证与完整训练数据部分代码与本部分代码除训练集外，写法基本一致，故略，见附件$hw3.py$）：

```python
for k in range(len(learning_rate)):#本部分对于每个学习率进行训练，并用验证集进行验证
    iter=[]
    loss_list=[]#列表用于记录损失函数值随迭代次数变化过程
    w = np.empty([163,1])
    for i in range(len(w)):
        w[i]=[1]
    adagrad = np.zeros([dim, 1])
    for t in range(iter_time):
        loss = np.sqrt(np.sum(np.power(np.dot(x_train_set, w) - y_train_set, 2))/ len(x_train_set))#均方根误差，RMSE
        #loss = np.sum(np.power(np.dot(x_train_set, w) - y_train_set, 2))#MSE
        if t % 10 == 0: #每10次迭代存一次值
            iter.append(t)
            loss_list.append(loss)
        gradient = 2 * np.dot(x_train_set.transpose(), np.dot(x_train_set, w) - y_train_set)  #RMSE 的偏微分 
        #gradient = 2 * np.dot(x_train_set.transpose(), np.dot(x_train_set, w) - y_train_set)/ len(x_train_set)#MSE
        adagrad += gradient ** 2       #adagrad 是历史梯度平方和的累加  
        w = w - learning_rate[k] * gradient / np.sqrt(adagrad + eps) #w按照学习率和gadient梯度下降
```

利用最终训练的w和代码``ans = np.dot(test_x, w)``得到预测数据ans。

## 结果分析与评估

### 训练结果与问题评估-1

利用划分好的训练集对四种学习率【0.01,0.1,1,10】分别迭代4000次，得到下列结果：

![image-20231028203025876](机器学习3.assets/image-20231028203025876.png)

对训练的迭代次数和$loss$值作图如下（图一为$RMSE$,图二为$MSE$）：

![image-20231028203116711](机器学习3.assets/image-20231028203116711.png)

![image-20231030191133536](机器学习3.assets/image-20231030191133536.png)

分析上述三个图，可以看到，看出不同的学习率对模型的性能有着明显的影响，下面针对RMSE的LOOS图像进行(1)(2)(3)的分析：

**（1）当学习率较小（0.01）时**，验证集损失较高（24.79），这可能是因为学习率过小导致模型收敛速度过慢，无法充分学习数据的特征。

同时，其loss值在1000-4000时下降的越来越慢直至接近水平，可能是因为学习率过小导致模型参数无法足够快速地跳出局部最优点，从而无法找到全局最优解。

**（2）当学习率增大到0.1和1时，**验证集损失显著减少至14.09和5.66，这表明增大学习率可以加快模型的收敛速度，使其更好地适应训练数据。

同时，当学习率为0.1时，可以看到其loss值下降地很慢，这印证了（1）中的结论，学习率过小会导致模型收敛速度慢。

**（3）在学习率为10时**，其loss值发生“变小--变大--变小”的过程，最后loss值略大于学习率为1的loss值，这说明过大的学习率可能导致模型在训练过程中跳过最优解。

同时，其loss值最终与学习率为1时非常接近（略大于学习率为1时），这说明学习率进一步增大并没有带来更好的结果。

**（4）不同的loss函数会影响模型的收敛速度**，但在本实验中，虽然loss的具体值相差很大（mse的值单位为$10^7$），但$RMSE$和$MSE$的训练过程基本一致.

### 问题评估-2

为了提高模型训练的准确度，我使用了以下方法：

**（1）进行特征提取时，综合考虑18种特征**：PM2.5可能与18种特征都有关系，单独考虑几种关系较大的特征不能很好的提升训练的准确度，故在本次实验中，提取了表格中全部18种特征，设置了18维的w（由于还有b，实际上是19维）的权重进行模型训练。

**（2）数据预处理**：对数值特征进行使用 Z-score 标准化，使数据符合标准正态分布，消除量纲影响，标准化后的数据具有相似的尺度和分布，这样可以使得模型更快地收敛到最优解。

此外，将表格中所以无法处理的NR替换成0。

**（3）使用$Adagrad$​优化**：可以根据每个参数的历史梯度信息自动调整学习率，对于频繁出现的参数梯度较大的特征，学习率会相应减小，以避免震荡或错过最优点。而对于不经常出现的参数梯度较小的特征，学习率会相应增大，以加速收敛。

**（4）将训练数据分为训练集和测试集**：使用不同的模型（本实验中是不同的学习率）对训练集进行训练，并使用验证集计算训练loss值，再利用整个训练数据和最佳的学习率进行重新训练。通过这种方法，优化模型的超参数(学习率)选择，从而提高模型的准确度。同时，确保在进行模型评估和选择时，使用独立的测试集来验证模型的泛化能力，以避免过拟合。

**(5)使用不同的$LOSS$函数：**使用了$RMSE$和$MSE$两者函数，但通过实际对比来看，两种函数在本次实验中对于准确度并无太大差异。

##  结论

不同的学习率对模型的训练速度，训练过程和训练结果有着影响。学习率过小导致模型收敛速度过慢，无法充分学习数据的特征，同时可能会导致模型参数无法跳出局部最优点。过大的学习率可能导致模型在训练过程中跳过最优解。综上，选择合适的学习率，可以加快模型的收敛速度，使其更好地适应训练数据。
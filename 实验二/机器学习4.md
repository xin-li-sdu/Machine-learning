# 机器学习-实验四

   

​                                                                                             李昕.202100460065




## 实验目的

在本次作业中，需要实现二分类，根据给出的个人信息，判断其年收入是否高于 50,000 美元。作业将用 logistic regression 与 generative model两种模型来实现目标，同时得到预测结果输出到结果文件中比较准确率。

## 实验环境

| 操作系统   | 处理器      | 使用语言 | 编译器            |
| ---------- | ----------- | -------- | ----------------- |
| windows 11 | AMD Ryzen 7 | python3  | Visual Studio2022 |

## 实验方法

### 概率生成模型

在概率生成模型中，根据贝叶斯公式可根据先验概率求后验概率：
$$
P(C_1|x)=\frac{P(x|C_1)P(C_1}{P(x|C_1)P(C_1)P(x|C_2)P(C_2)}
$$
利用最大似然估计计算$P(x|C_1)$和$P(x|C_2)$，利用训练数据集中数据计算 $\mu=\frac{1}{N}\sum x^2$和$\sum=\frac{1}{N}\sum(x^n-\overline{x})(x^n-\overline{x})^T$，并计算共享协方差矩阵:
$$
\sum=\frac{N_1}{N}\sum^1+ \frac{N_2}{N}\sum^2
$$
利用求得的$\mu$和$\sum$计算参数w和b：
$$
\begin{array}{c} \mathrm{w}^{\mathrm{T}}=\left(\mu^{0}-\mu^{1}\right)^{\mathrm{T}} \Sigma^{-1} \\ \mathrm{~b}=-\frac{1}{2}\left(\mu^{0}\right)^{\mathrm{T}} \Sigma^{-1} \mu^{0}+\frac{1}{2}\left(\mu^{1}\right)^{\mathrm{T}} \Sigma^{-1} \mu^{1}+\ln \frac{\mathrm{n}_{0}}{\mathrm{n}_{1}} \end{array}
$$
后验概率公式可以推到为：
$$
P\left(C_{1} \mid x\right)=\frac{1}{1+\frac{P\left(x \mid C_{2}\right) P\left(C_{2}\right)}{P\left(x \mid C_{1}\right) P\left(C_{1}\right)}}\\=\frac{1}{1+\exp (-z)}\ ,\ z=\ln \frac{P\left(x \mid C_{1}\right) P\left(C_{1}\right)}{P\left(x \mid C_{2}\right) P\left(C_{2}\right)}
$$
根据$P\left(C_{1} \mid x\right)$与0.5的比较来进行二分类。

### 逻辑回归模型

逻辑回归假设数据服从伯努利分布（即数据的标签为0或者1），通过极大化似然函数的方法，运用梯度下降来求解参数，来达到将数据二分类的目的。

当共享协方差均值时，并使用另一个假设分布（朴素贝叶斯）。可以得到如下公式(Sigmoid)：
$$
\begin{array}{c} P_{w, b}\left(C_{1} \mid x\right)=\sigma(z)=\frac{1}{1+e^{-z}} \\ z=w \cdot x+b=\sum_{i} w_{i} x_{i}+b \end{array}
$$
我们可以尝试直接训练参数（w,b）提取特征计算误差最小化的参数，引入交叉熵计算 loss：
$$
-lnL(w,b)=-\sum(\hat{y}^nlnf_{w,b}(x^n)+(1-\hat{y}^n)ln(1-f_{w,b}(x^n)))
$$
对应的偏微分公式为：
$$
\begin{array}{l} \frac{\partial(-\ln \mathrm{L}(\mathrm{w}, \mathrm{b}))}{\partial \mathrm{w}_{\mathrm{i}}}=-\sum_{\mathrm{n}}\left(\hat{\mathrm{y}}^{\mathrm{n}}-\mathrm{fw}, \mathrm{b}\left(\mathrm{x}^{\mathrm{n}}\right)\right) \mathrm{x}_{\mathrm{i}}^{\mathrm{n}} \\ \frac{\partial(-\ln \mathrm{L}(\mathrm{w}, \mathrm{b}))}{\partial \mathrm{b}}=-\sum_{\mathrm{n}}\left(\hat{\mathrm{y}}^{\mathrm{n}}-\mathrm{fw}, \mathrm{b}\left(\mathrm{x}^{\mathrm{n}}\right)\right) \end{array}
$$
梯度下降的公式为（使用$Adagrad$方法更新学习率）：
$$
\begin{aligned} \mathrm{w}_{\mathrm{i}} &=\mathrm{w}_{\mathrm{i}}-\frac{\eta}{\sqrt {\sum^{n-1}_{i=1}g^2_i}}\left(-\sum_{\mathrm{n}}\left(\hat{\mathrm{y}}^{\mathrm{n}}-\mathrm{fw}, \mathrm{b}\left(\mathrm{x}^{\mathrm{n}}\right)\right) \mathrm{x}_{\mathrm{i}}^{\mathrm{n}}\right) \\ \mathrm{b}_{\mathrm{i}} &=\mathrm{b}_{\mathrm{i}}-\frac{\eta}{\sqrt {\sum^{n-1}_{i=1}g^2_i}}\left(-\sum_{\mathrm{n}}\left(\hat{\mathrm{y}}^{\mathrm{n}}-\mathrm{fw}, \mathrm{b}\left(\mathrm{x}^{\mathrm{n}}\right)\right)\right) \end{aligned}
$$

## 实验过程

### 概率生成模型

#### 读取数据

在data_manager()类中定义了函数read()进行数据读取和Z-Score标准化处理：

```python
 def read(self,name,path):
        with open(path,newline = '') as csvfile:
            rows = np.array(list(csv.reader(csvfile))[1:] ,dtype = float) #去掉首行读取
            if name == 'X_train':
                    self.mean = np.mean(rows,axis = 0).reshape(1,-1)
                    self.std = np.std(rows,axis = 0).reshape(1,-1)+0.00000000000000001
                    self.theta = np.ones((rows.shape[1] + 1,1),dtype = float)
                    for i in range(rows.shape[0]):#对数据进行 z-score标准化，（x-μ）/σ
                        rows[i,:] = (rows[i,:]-self.mean)/(self.std)
            elif name == 'X_test': 
                    for i in range(rows.shape[0]):
                        rows[i,:] = (rows[i,:]-self.mean)/(self.std）
            self.data[name] = rows
```

#### **计算均值/共同协方差，并计算参数w和b**

按照实验方法中给出的公式，求两个分类均值和协方差，并计算共同协方差：

```python
	    mean_0 = np.mean(class_0,axis = 0)
        mean_1 = np.mean(class_1,axis = 0)  #分别求出两个分类均值
        cov_0 = np.cov(class_0, rowvar=False)
        cov_1 = np.cov(class_1, rowvar=False) #不同的类别共享协方差矩阵
        cov = (cov_0 * class_0.shape[0] + cov_1 * class_1.shape[0]) / (row_total_num)
        #两个类共享一个协方差,最终cov就是协方差
```

同时，利用均值和协方差，计算参数w和b：

```python
self.w =  np.dot((mean_0-mean_1).transpose(),inv(cov)).transpose()
self.b =(-1)*np.dot(np.dot(mean_0.transpose(),inv(cov)),mean_0)/2
+np.dot(np.dot(mean_1.transpose(),inv(cov)),mean_1)/2+np.log(float(n_0)/n_1)
```

#### 后验概率函数sigmoid

根据公式$P\left(C_{1} \mid x\right)=\frac{1}{1+\exp (-z)}\ ,\ z=\ln \frac{P\left(x \mid C_{1}\right) P\left(C_{1}\right)}{P\left(x \mid C_{2}\right) P\left(C_{2}\right)}$，得到以下代码，其中expit(z)等价于1 / (1 + np.exp(-z))，作用是防止np.exp()会溢出：

```python
def func(self,x):
        arr = np.empty([x.shape[0],1],dtype=float)
        for i in range(x.shape[0]):
            z = x[i,:].dot(self.w) + self.b
            arr[i][0] =expit(z)
        return np.clip(arr, 1e-8, 1-(1e-8))
```

#### 计算出生成模型在训练集上的准确率：

用求得的w和b计算sigmoid()函数结果，经过predict预测函数处理得到预测值，从而计算训练集上的准确率如下代码所示：

```python
result = self.func(self.data['X_train'])#经过预测函数处理得到预测值
answer = self.predict(result)
accuracy=1-np.mean(np.abs(self.data['Y_train']-answer))#计算训练集上的准确率
```

得到以下结果：

![image-20231110155750839](image-20231110155750839.png)

### 逻辑回归模型

#### 把数据分成训练集和验证集

首先，将数据分成训练集和验证集，进行交叉验证：

```python
def partition(X, Y, verify_ratio):
    train_size = int(len(X) * (1 - verify_ratio))
    return X[:train_size], Y[:train_size], X[train_size:], Y[train_size:]
```

#### Adagrad梯度下降

首先定义交叉熵函数，用来计算loss值，根据公式得到以下代码：

```python
def loss(y_pred, Y_label):#交叉熵
    cross_entropy = -np.dot(Y_label, np.log(y_pred)) - np.dot((1 - Y_label), np.log(1 - y_pred))
    return cross_entropy
```

之后进行迭代训练，使用Adagrad动态调整学习率，gradient()函数实现利用偏微分计算下降梯度

```python
def gradient(X, Y_label, w, b):#逻辑回归下降梯度
    y_pred = func(X, w, b)
    pred_error = Y_label - y_pred
    w_grad = -np.sum(pred_error * X.T, 1)
    b_grad = -np.sum(pred_error)
    return w_grad, b_grad
for i in tqdm(range(iteration)):
        w_grad, b_grad = gradient(X_train, Y_train, w, b)#计算梯度
        w_adagrad += (w_grad) ** 2
        b_adagrad += (b_grad) ** 2
        w = w - learning_rate/np.sqrt(w_adagrad+eps) * w_grad# 梯度下降法更新
        b = b - learning_rate/np.sqrt(b_adagrad+eps) * b_grad
```

#### 计算训练集和验证集的loss和accuracy

利用4.2.2定义的loss()函数计算loss值，同时利用4.1.4定义的accuracy()函数计算训练集和验证集上的正确率，并作图。

得到以下结果：

![image-20231110155710024](image-20231110155710024.png)

![image-20231110155611968](image-20231110155611968.png)

![image-20231110155632153](image-20231110155632153.png)

### Regularization正则化的影响

引入$L2$正则化，损失函数修改为：$L=L(W)+\lambda \sum_{i=1}^n w_i^2$,代码修改为：

```python
def loss(y_pred, Y_label,w):#交叉熵
    r= lamb * np.sum(w ** 2) 
    cross_entropy = -np.dot(Y_label, np.log(y_pred)) - np.dot((1 - Y_label), np.log(1 - y_pred))
    cross_entropy+=r
    return cross_entropy
```

对于梯度，有：
$$
\frac{\partial L}{\partial w_i}=\frac{\partial L(W)}{\partial w_i}+2\lambda w_i\\ w_{i+1}=w_i-\eta \frac{\partial L(W)}{\partial w_i}-\eta 2\lambda w_i
$$
在梯度下降过程中添加如下语句即可：

```python
 w_grad += 2*learning_rate*lamb * w  #w梯度加入正则项。
```

运行得到新的loss值和准确率：

![image-20231114194153512](image-20231114194153512.png)

### 判断影响最大的特征

对w的绝对值从大到小排序，权重越大影响越大，可以看到“性别”这个特征的系数绝对值为2.33，影响年收入最大的特征。

![image-20231114200501987](image-20231114200501987.png)

## 结果分析与评估

### 不同模型对准确度的影响

| 模型名称     | 准确度       |
| ------------ | ------------ |
| 概率生成模型 | 0.8424188446 |
| 逻辑回归模型 | 0.8514537264 |

可以看到逻辑回归模型相较于概率生成模型的准确度更高，但差距不大。

### **特征标准化对模型准确率的影响**

 在训练时，如果某个特征的取值范围比其他特征大很多，那么数值计算就受该特征的主要支配。 但实际上并不一定是这个特征最重要。标准化数据可以使不同维度的特征放在一起进行比较，可以大大提高模型的准确性。

去掉逻辑回归模型的标准化，得到以下结果：

![image-20231114201746564](image-20231114201746564.png)

与有标准化进行比较，得到下表：

| 是否使用标准化 | 准确度       | 权值最大的特征 |
| -------------- | ------------ | -------------- |
| 否             | 0.7853535353 | 性别           |
| 是             | 0.8514537264 | 学位           |

可以看到准确率下降较多，且此时占主导地位的特征变成了""Bachelors"。

### 正则化对模型准确率的影响

| 是否使用正则化 | 准确度       |
| -------------- | ------------ |
| 否             | 0.8514537264 |
| 是             | 0.8517608517 |

由上表可以看出，是否正则化对正确率的影响非常小，使用正则化很微弱的改善了训练效果，但是提升的效果不明显,分析其原因是正则化对模型具有高次项时的过拟合现象有较好的改良效果，但本实验中为线性模型，故其影响很小。

### **猜测对结果影响最大的特征**

在4.4中输出了权值最大的特征，在本实验训练的模型中，权值最大特征是性别sex，其系绝对值数为2.3317。

##  结论

根据本实验的上述过程，可以得出以下结论：

1. 在所比较的概率生成模型和逻辑回归模型中，逻辑回归模型的准确度高于概率生成模型，但两者差距不大。
2. 特征标准化对模型的准确率影响显著，标准化数据可以提高模型的准确性，特别是在处理不同数据量级的特征时更为重要。
3. 使用正则化对本实验线性模型准确率的影响相对较小，在本实验中正则化略微改善了训练效果，但提升效果不明显。
4. 性别（sex）是对本模型结果影响最大的特征。
